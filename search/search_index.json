{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"KubeLens Documentation","text":"<p>KubeLens is a read-only Kubernetes log analyzer that provides a focused UI for logs and resource inspection without full cluster management access.</p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Quickstart</li> <li>Backend Runtime Environment</li> <li>Deployment Guide</li> <li>Configuration</li> <li>Architecture</li> <li>Screenshots</li> <li>Changelog</li> <li>GitHub Pages Publishing</li> </ul>"},{"location":"#project-layout","title":"Project Layout","text":"<ul> <li><code>frontend/</code> React + Vite UI</li> <li><code>backend/</code> Go API server (auth, sessions, Kubernetes proxy)</li> <li><code>docker/</code> Dockerfile, nginx, supervisor, and compose assets</li> <li><code>charts/</code> Helm chart (namespace: <code>monitoring</code>)</li> <li><code>deploy/</code> Kustomize manifests (namespace: <code>monitoring</code>)</li> <li><code>refs/</code> product and backend specs</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>KubeLens runs as a single pod (frontend + backend + nginx) and exposes a single HTTP entrypoint.</p>"},{"location":"architecture/#components","title":"Components","text":"<ul> <li>Frontend: React + Vite SPA served by nginx.</li> <li>Backend: Go service that handles authentication, session storage, and Kubernetes API proxying.</li> <li>Nginx: Serves static assets and proxies <code>/api/*</code> requests to the backend.</li> <li>Supervisor: Manages the nginx and backend processes inside the container.</li> </ul>"},{"location":"architecture/#data-flow","title":"Data flow","text":"<p>1) User loads the SPA from nginx. 2) SPA redirects to Keycloak for auth if needed. 3) Backend validates JWTs and applies namespace/label filters. 4) Kubernetes resources are cached via shared informers and short\u2011TTL snapshots to reduce API load. 5) Logs stream via SSE from the backend to the frontend. Log workers are pooled per pod/container and can optionally use Redis Streams to share a single upstream stream across backend replicas. 6) User preferences persist via the backend session store.</p>"},{"location":"architecture/#auth-config-handshake","title":"Auth config handshake","text":"<p>The frontend loads Keycloak settings at runtime from <code>GET /api/v1/auth/config</code>. The response is cached locally for a few minutes to reduce repeated calls, and the UI only falls back to build-time <code>VITE_KEYCLOAK_*</code> overrides if the backend endpoint is unavailable.</p>"},{"location":"architecture/#config-hot-reload","title":"Config hot reload","text":"<p>When the backend is configured via a mounted ConfigMap, it watches the config file for changes and reloads the runtime configuration without a restart.</p>"},{"location":"architecture/#observability","title":"Observability","text":"<ul> <li>Cache activity metrics are exposed at <code>GET /api/v1/metrics</code>.</li> <li>Backend logs are structured and colored using Charmbracelet <code>log</code>.</li> </ul>"},{"location":"changelogs/","title":"Changelog","text":""},{"location":"changelogs/#v004-2026-02-12","title":"v0.0.4 - 2026-02-12","text":"<ul> <li>Auth: centralized frontend 401 handling now clears local auth artifacts and triggers a clean Keycloak sign-in redirect.</li> <li>Auth: automatic sign-in loop guard halts redirects after repeated failures and shows a manual retry path.</li> <li>UI: post-login navigation now restores the prior <code>#view=...</code> hash after re-authentication.</li> <li>Streaming: log SSE no longer reconnects forever on expired tokens; it delegates to the global auth expiry flow.</li> </ul>"},{"location":"changelogs/#v003-2026-01-31","title":"v0.0.3 - 2026-01-31","text":"<ul> <li>Per-namespace log rate limit overrides for high-traffic namespaces.</li> <li>Metrics refresh loop with stale marking and human-friendly CPU/memory formatting.</li> <li>CRD selector path support for custom app resources.</li> <li>Saved-view log include/exclude regex filters.</li> <li>Log context modal (\u00b120 lines), timeline scrubber + jump-to-time UI.</li> <li>Compare mode for side-by-side pod log inspection.</li> <li>Stream reconnect + dropped-line toasts.</li> </ul>"},{"location":"changelogs/#v002-2026-01-31","title":"v0.0.2 - 2026-01-31","text":"<ul> <li>Saved views with namespace/label/group/level presets and per-user persistence.</li> <li>Global log search with highlight + jump, plus per-stream wrap/time/detail overrides.</li> <li>Stream health indicator with reconnect/lag stats and backpressure counters.</li> <li>Pod lifecycle markers inline (added/removed/ready/restart) for app streams.</li> <li>Container switcher for multi-container apps and log line annotations + permalinks.</li> <li>Log stream rate limiting and structured audit logging.</li> <li>Config validation endpoint (<code>/api/v1/config/validate</code>) and UI enhancements for stream sources.</li> </ul>"},{"location":"changelogs/#v001-2026-01-30","title":"v0.0.1 - 2026-01-30","text":"<ul> <li>Initial release of KubeLens MVP (read-only Kubernetes log analyzer).</li> <li>SSE log streaming with pooled workers and optional Redis Streams fan-out.</li> <li>App/pod list caching (informers + TTL) with metadata-only mode.</li> <li>Keycloak SSO with group-based access control and masked secrets.</li> <li>Server-side session persistence (redis/sqlite/postgres).</li> <li>Docker all-in-one runtime with nginx + supervisor and optional local Valkey.</li> <li>Helm chart + Kustomize manifests for in-cluster deployment.</li> <li>MkDocs Material documentation site with deployment guides.</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>KubeLens uses a YAML config file for backend behavior. See <code>backend/config.example.yaml</code> for the full schema.</p>"},{"location":"configuration/#highlights","title":"Highlights","text":"<ul> <li>Auth: Keycloak OIDC with required group membership.</li> <li>Logs: Defaults to 10,000 tail lines with 10,000 character max line length. App log streams resync pod membership every 10s by default.</li> <li>Session storage: Redis, sqlite, or postgres.</li> <li>Kubernetes: Namespace allowlist, app grouping labels, include/exclude filters.</li> </ul>"},{"location":"configuration/#frontend-auth-config","title":"Frontend auth config","text":"<p>The frontend reads Keycloak settings at runtime from: <pre><code>GET /api/v1/auth/config\n</code></pre> This endpoint returns the Keycloak URL, realm, client ID, and allowed groups from the backend config. It does not return secrets. The UI caches this response locally for a few minutes and will only fall back to build-time <code>VITE_KEYCLOAK_*</code> overrides if the endpoint is unavailable.</p> <p>Note: KubeLens expects a <code>groups</code> claim in the access token. In Keycloak, add the Group Membership mapper (client scope <code>groups</code>) to the <code>kubelens</code> client and include the <code>groups</code> scope in the auth request.</p>"},{"location":"configuration/#session-expiry-behavior","title":"Session expiry behavior","text":"<p>When the backend returns <code>401 Unauthorized</code> (for example, expired access token), the frontend now performs an auth reset flow: - Clears the cached access token and best-effort auth cookies on the KubeLens domain. - Redirects to Keycloak login automatically. - Preserves the current URL hash (<code>#view=...</code>) and restores it after successful login. - Applies a loop guard (max 3 automatic redirects in 2 minutes); if exceeded, auto-redirect stops and the user can retry manually from the login error UI.</p>"},{"location":"configuration/#log-stream-tuning","title":"Log stream tuning","text":"<p><pre><code>logs:\n  app_stream_resync_seconds: 10\n</code></pre> This controls how often app log streams re-check pod membership to pick up new replicas or rolling updates.</p>"},{"location":"configuration/#resource-metrics-cpumemory","title":"Resource metrics (CPU/Memory)","text":"<p>KubeLens fetches live usage from the Kubernetes Metrics API (<code>metrics.k8s.io</code>). Ensure <code>metrics-server</code> is installed in the cluster. The frontend requests metrics on demand via the <code>metrics=true</code> query parameter. When metrics are unavailable, usage fields render as <code>\u2014</code>.</p> <p>Background refresh and staleness thresholds: <pre><code>kubernetes:\n  api_cache:\n    metrics_list_ttl_seconds: 5\n    metrics_refresh_seconds: 15\n    metrics_refresh_jitter_seconds: 5\n    metrics_stale_seconds: 30\n</code></pre></p>"},{"location":"configuration/#shared-log-workers-redis-streams","title":"Shared log workers (Redis Streams)","text":"<p>KubeLens can pool log streams across multiple backend replicas using Redis Streams: <pre><code>logs:\n  use_redis_streams: true\n  redis_stream_prefix: \"kubelens:logs\"\n  redis_stream_maxlen: 10000\n  redis_stream_block_millis: 2000\n  redis_lock_ttl_seconds: 15\n  redis_url: \"\" # optional override; defaults to cache.redis_url\n</code></pre> When enabled, each pod/container log stream is handled by a single leader that writes to Redis, while other replicas read and fan out to their subscribers. This reduces upstream Kubernetes log streams and improves team-scale usage. The in-process log worker also maintains a ring buffer (default 10k lines) to support fast replay for reconnecting clients.</p>"},{"location":"configuration/#log-stream-rate-limiting","title":"Log stream rate limiting","text":"<p>To avoid excessive log stream opens per user/namespace: <pre><code>logs:\n  rate_limit_per_minute: 120\n  rate_limit_burst: 240\n  rate_limit_overrides:\n    - namespace: \"internal-apps\"\n      rate_limit_per_minute: 240\n      rate_limit_burst: 480\n</code></pre> Limits apply per user + namespace and return <code>429</code> when exceeded.</p>"},{"location":"configuration/#audit-logging","title":"Audit logging","text":"<p>Enable structured audit logs for key actions (logs, reads, lists): <pre><code>server:\n  audit_logs: true\n</code></pre></p>"},{"location":"configuration/#custom-resources","title":"Custom resources","text":"<p>You can add additional CRDs to the Apps view via config: <pre><code>kubernetes:\n  custom_resources:\n    - name: \"cnpg\"\n      group: \"postgresql.cnpg.io\"\n      version: \"v1\"\n      resource: \"clusters\"\n      kind: \"Cluster\"\n      enabled: true\n      pod_label_key: \"cnpg.io/cluster\"\n      selector_path: \"spec.selector\"\n</code></pre> <code>pod_label_key</code> is optional but required for log streaming to work for the CRD. <code>selector_path</code> is optional and lets you point to the CRD's label selector (e.g. <code>spec.selector</code> or <code>spec.selector.matchLabels</code>); if omitted, KubeLens tries common selector paths.</p>"},{"location":"configuration/#api-cache-modes","title":"API cache modes","text":"<p>The API cache supports an optional metadata-only list mode to reduce API server load: <pre><code>kubernetes:\n  api_cache:\n    metadata_only: true\n</code></pre> When enabled, list endpoints return <code>metadataOnly: true</code> resources with minimal fields, and the UI fetches full resource details on demand.</p> <p>Warm caches on startup (optional): <pre><code>kubernetes:\n  api_cache:\n    warm_on_startup: true\n</code></pre></p> <p>Metrics responses are cached briefly to reduce metrics-server load: <pre><code>kubernetes:\n  api_cache:\n    metrics_list_ttl_seconds: 5\n</code></pre></p> <p>Cache metrics are exposed at: <pre><code>GET /api/v1/metrics\n</code></pre></p> <p>Configuration validation is available at: <pre><code>GET /api/v1/config/validate\n</code></pre></p>"},{"location":"configuration/#sse-timeouts","title":"SSE timeouts","text":"<p>Log streaming uses long-lived SSE connections. Set: <pre><code>server:\n  write_timeout_seconds: 0\n</code></pre> to disable the write timeout so streams are not terminated mid-session.</p>"},{"location":"configuration/#local-cache-in-the-container","title":"Local cache in the container","text":"<p>When using the single-container image, you can optionally start a local Valkey instance with: <pre><code>START_LOCAL_VALKEY=true\nLOCAL_VALKEY_DATA_DIR=/data/cache\nLOCAL_VALKEY_MAXMEMORY=512mb\n</code></pre> If no <code>LOCAL_VALKEY_MAXMEMORY</code> is provided, Valkey auto-tunes maxmemory to ~70% of the container\u2019s memory limit when available.</p>"},{"location":"configuration/#example","title":"Example","text":"<pre><code>auth:\n  keycloak_url: \"https://keycloak.enterprise.com\"\n  realm: \"monitoring\"\n  client_id: \"kubelens\"\n  client_secret: \"REDACTED\"\n  allowed_groups:\n    - \"k8s-logs-access\"\n  allowed_secrets_groups:\n    - \"k8s-admin-access\"\n\nstorage:\n  database_url: \"sqlite://./data/kubelens.sqlite\"\n\ncache:\n  enabled: true\n  redis_url: \"redis://localhost:6379/0\"\n\nkubernetes:\n  cluster_name: \"enterprise-cluster\"\n  api_cache:\n    enable_informers: true\n    informer_resync_seconds: 30\n    pod_list_ttl_seconds: 2\n    app_list_ttl_seconds: 5\n    crd_list_ttl_seconds: 10\n    retry_attempts: 3\n    retry_base_delay_ms: 200\n    metadata_only: false\n  allowed_namespaces:\n    - \"apps\"\n    - \"db\"\n  app_groups:\n    enabled: true\n    labels:\n      selector: \"app.enterprise.com/name\"\n      name: \"app.enterprise.com/displayname\"\n      environment: \"app.enterprise.com/env\"\n      version: \"app.enterprise.com/version\"\n</code></pre>"},{"location":"deploy/","title":"Deployment Guide","text":""},{"location":"deploy/#container-image","title":"Container image","text":"<p>Build the container image: <pre><code>docker build -f docker/Dockerfile -t halceon/kubelens .\n</code></pre></p>"},{"location":"deploy/#runtime-model-single-pod","title":"Runtime model (single pod)","text":"<p>The container runs: - Nginx serving the frontend and proxying <code>/api/*</code> to the backend. - Go backend listening on <code>:8080</code>. - Supervisor managing both processes.</p>"},{"location":"deploy/#required-config","title":"Required config","text":"<p>Mount your config file and kubeconfig:</p> <ul> <li><code>/etc/kubelens/config.yaml</code></li> <li><code>/etc/kubeconfig</code></li> </ul> <p>Environment variables: <pre><code>KUBELENS_CONFIG=/etc/kubelens/config.yaml\nKUBECONFIG=/etc/kubeconfig\n</code></pre></p>"},{"location":"deploy/#example-docker-compose","title":"Example (docker compose)","text":"<pre><code>docker compose -f docker/docker-compose.yml up --build\n</code></pre>"},{"location":"deploy/#kubernetes-notes","title":"Kubernetes notes","text":"<ul> <li>Run the container as a single pod behind a Service.</li> <li>Mount the config file via ConfigMap and the kubeconfig via Secret.</li> <li>Keep the pod read-only: no cluster write permissions.</li> </ul>"},{"location":"deploy/#helm-namespace-monitoring","title":"Helm (namespace: monitoring)","text":"<pre><code>helm install kubelens charts/kubelens -n monitoring --create-namespace\n</code></pre> <p>Update values (Keycloak + namespaces) in <code>charts/kubelens/values.yaml</code> or with <code>--set</code>.</p>"},{"location":"deploy/#kustomize-namespace-monitoring","title":"Kustomize (namespace: monitoring)","text":"<pre><code>kubectl apply -k deploy\n</code></pre> <p>Edit <code>deploy/config/kubelens-config.yaml</code> to match your Keycloak and namespace allowlist before applying.</p>"},{"location":"github-pages/","title":"GitHub Pages Publishing","text":""},{"location":"github-pages/#enable-github-pages","title":"Enable GitHub Pages","text":"<p>1) In the repository settings, open Pages. 2) Set Source to GitHub Actions. 3) Save. GitHub will publish the site at: <pre><code>https://halceonio.github.io/kubelens/\n</code></pre></p>"},{"location":"github-pages/#local-preview-optional","title":"Local preview (optional)","text":"<p>Docs are built with MkDocs Material. Preview locally with: <pre><code>make docs-preview\n</code></pre></p>"},{"location":"github-pages/#notes","title":"Notes","text":"<ul> <li>The docs entry point is <code>docs/index.md</code>.</li> <li>The Pages build is handled by <code>.github/workflows/docs.yml</code>.</li> </ul>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#local-development","title":"Local development","text":"<p>1) Configure the backend: <pre><code>cp backend/config.example.yaml backend/config.yaml\nexport KUBELENS_CONFIG=backend/config.yaml\nexport KUBECONFIG=~/.kube/config\n</code></pre></p> <p>2) (Optional) Configure the frontend for mocks or a dev token: <pre><code>cp frontend/.env.example frontend/.env\n</code></pre></p> <p>3) Run both services: <pre><code>make dev\n</code></pre></p> <p>4) Open the UI: <pre><code>http://localhost:3000\n</code></pre></p>"},{"location":"quickstart/#docker-single-container-runtime","title":"Docker (single-container runtime)","text":"<pre><code>cp backend/config.example.yaml docker/config.yaml\n# Place a kubeconfig at docker/kubeconfig for local testing\ndocker compose -f docker/docker-compose.yml up --build\n</code></pre> <p>Open: <pre><code>http://localhost:8080\n</code></pre></p>"},{"location":"runtime-env/","title":"Backend Runtime Environment","text":"<p>The backend is configured via YAML and a small set of environment variables that point to configuration and kubeconfig files.</p>"},{"location":"runtime-env/#required-production","title":"Required (production)","text":"<ul> <li><code>KUBELENS_CONFIG</code> or <code>KUBELENS_CONFIG_PATH</code>: absolute or relative path to the YAML config file.</li> </ul>"},{"location":"runtime-env/#optional-local-test","title":"Optional (local / test)","text":"<ul> <li><code>KUBELENS_KUBECONFIG</code>: path to a kubeconfig file used for local development.</li> <li><code>KUBECONFIG</code>: standard Kubernetes kubeconfig env var (used if <code>KUBELENS_KUBECONFIG</code> is not set).</li> </ul>"},{"location":"runtime-env/#optional-container-cache-bootstrap","title":"Optional (container cache bootstrap)","text":"<p>When running the all-in-one container, you can start a local Valkey (Redis-compatible) instance: - <code>START_LOCAL_VALKEY</code> or <code>START_LOCAL_REDIS</code>: set to <code>true</code>, <code>1</code>, or <code>yes</code> to enable. - <code>LOCAL_VALKEY_DATA_DIR</code> or <code>LOCAL_REDIS_DATA_DIR</code>: data directory for Valkey (default <code>/data/cache</code>). - <code>LOCAL_VALKEY_MAXMEMORY</code> or <code>LOCAL_REDIS_MAXMEMORY</code>: optional max memory setting (e.g. <code>512mb</code> or bytes). - <code>KUBELENS_CACHE_ENABLED</code>: optional override for cache.enabled (set automatically when local Valkey starts). - <code>KUBELENS_CACHE_REDIS_URL</code>: optional override for cache.redis_url (set automatically when local Valkey starts).</p> <p>If a local cache is enabled, the entrypoint forces: <pre><code>cache:\n  enabled: true\n  redis_url: \"redis://localhost:6379/0\"\n</code></pre></p> <p>Maxmemory auto-tuning: if <code>LOCAL_*_MAXMEMORY</code> is not set and a container memory limit is detected, the entrypoint sets Valkey maxmemory to ~70% of the cgroup limit. If no limit is detected, Valkey runs without a maxmemory cap.</p>"},{"location":"runtime-env/#defaults","title":"Defaults","text":"<p>If no env vars are set, the backend will look for config files in this order: 1) <code>KUBELENS_CONFIG</code> or <code>KUBELENS_CONFIG_PATH</code> 2) <code>/etc/kubelens/config.yaml</code> 3) <code>./config.yaml</code> 4) <code>./backend/config.yaml</code></p> <p>If no kubeconfig is set, the backend defaults to in-cluster configuration.</p>"},{"location":"screenshots/","title":"Screenshots","text":"<p>These placeholders get replaced with validated UI screenshots from the live cluster.</p>"},{"location":"screenshots/#groups-view-type-badges","title":"Groups view (type badges)","text":""},{"location":"screenshots/#apps-view-crds-included","title":"Apps view (CRDs included)","text":""},{"location":"screenshots/#pod-logs","title":"Pod logs","text":""},{"location":"screenshots/#app-logs","title":"App logs","text":""}]}